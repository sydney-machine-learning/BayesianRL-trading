{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(101)\n",
    "np.random.seed(101)\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "# print('  - low:', env.action_space.low)\n",
    "# print('  - high:', env.action_space.high)\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env, h_size=20):\n",
    "        super(Agent, self).__init__()\n",
    "        self.env = env\n",
    "        # state, hidden layer, action sizes\n",
    "        self.s_size = env.observation_space.shape[0]\n",
    "        self.h_size = h_size\n",
    "        self.a_size = 2\n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        s_size = self.s_size\n",
    "        h_size = self.h_size\n",
    "        a_size = self.a_size\n",
    "        # separate the weights for each layer\n",
    "        fc1_end = (s_size*h_size)+h_size\n",
    "        fc1_W = torch.from_numpy(weights[:s_size*h_size].reshape(s_size, h_size))\n",
    "        fc1_b = torch.from_numpy(weights[s_size*h_size:fc1_end])\n",
    "        fc2_W = torch.from_numpy(weights[fc1_end:fc1_end+(h_size*a_size)].reshape(h_size, a_size))\n",
    "        fc2_b = torch.from_numpy(weights[fc1_end+(h_size*a_size):])\n",
    "        # set the weights for each layer\n",
    "        self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))\n",
    "        self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))\n",
    "    \n",
    "    def get_weights_dim(self):\n",
    "        return (self.s_size+1)*self.h_size + (self.h_size+1)*self.a_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x=F.softmax(x,dim=0)\n",
    "        \n",
    "        return x.cpu().data\n",
    "        \n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.set_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        prob_value=1\n",
    "        variance=[]\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action=self.forward(state)\n",
    "            act=action.numpy()\n",
    "#             print(\"action = \",action)\n",
    "            E=(act[0]*-1 )+(act[1]*1)\n",
    "            var= act[0]+act[1]-(E**2)\n",
    "            values,indices=torch.max(action,0)\n",
    "            prob_value*=values\n",
    "            action=indices.item()\n",
    "            variance.append(var)\n",
    "            \n",
    "            \n",
    "            \n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "          #  print(\"timesteps=\",t)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "          #  print(\"cum reward right now\",episode_return)\n",
    "            if done:\n",
    "                break\n",
    "#       print(variance)\n",
    "        loss=np.mean(variance)\n",
    "#         print(\"loss=\",loss)\n",
    "        return loss\n",
    "    def evaluate_return(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.set_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        prob_value=1\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action=self.forward(state)\n",
    "            \n",
    "            values,indices=torch.max(action,0)\n",
    "            prob_value*=values\n",
    "            action=indices.item()\n",
    "            \n",
    "            \n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            #print(\"step score\",reward)\n",
    "            #print(\"timesteps=\",t)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "           # print(\"cum reward right now\",episode_return)\n",
    "            if done:\n",
    "                break\n",
    "        return (episode_return)\n",
    "    \n",
    "agent = Agent(env).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.s_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_likelihood(sigma_squared, nu_1, nu_2, w, tausq,h,d):\n",
    "    h = h  # number hidden neurons\n",
    "    d = d  # number input neurons\n",
    "    print(np.shape(w)[0])\n",
    "    part1 = -1 * ((np.shape(w)[0]) / 2) * np.log(sigma_squared)\n",
    "    part2 = 1 / (2 * sigma_squared) * (sum(np.square(w)))\n",
    "    log_loss = part1 - part2 - (1 + nu_1) * np.log(tausq) - (nu_2 / tausq)\n",
    "    return log_loss\n",
    "\n",
    "def likelihood_func(sigma,gamma, max_t,w, tausq,n_iterations,loss):\n",
    "        #y = data[:, self.topology[0]]\n",
    "        \n",
    "        rmse = np.mean(loss)\n",
    "        loss = -0.5 * np.log(2 * math.pi * tausq) - 0.5 * np.square(loss) / tausq\n",
    "        return [np.sum(loss), rmse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "142\n",
      "-12.819982472547025\n",
      "142\n",
      "-15.7110383141586\n",
      "142\n",
      "-14.847249469157134\n",
      "142\n",
      "-10.96150956878904\n",
      "142\n",
      "-11.283070602546331\n",
      "142\n",
      "-14.410531449507147\n",
      "142\n",
      "-12.6587055368162\n",
      "142\n",
      "-13.175056566770479\n",
      "142\n",
      "-14.436905676144221\n",
      "142\n",
      "-13.234319811811076\n",
      "Episode 10\tAverage Score: 9.60\n",
      "142\n",
      "-13.347116914962015\n",
      "142\n",
      "-10.18221557279793\n",
      "142\n",
      "-14.288209628614908\n",
      "142\n",
      "-15.789059674583541\n",
      "142\n",
      "-10.817716188969598\n",
      "142\n",
      "-10.734175423229033\n",
      "142\n",
      "-12.797628165199995\n",
      "142\n",
      "-12.849040613716511\n",
      "142\n",
      "-12.315393714485623\n",
      "142\n",
      "-13.18798901929199\n",
      "Episode 20\tAverage Score: 9.50\n",
      "142\n",
      "-10.726304268469676\n",
      "142\n",
      "-13.883473872611887\n",
      "142\n",
      "-10.22294721712305\n",
      "142\n",
      "-9.880368788749394\n",
      "142\n",
      "-11.789861330432245\n",
      "142\n",
      "-11.327685876753208\n",
      "142\n",
      "-10.447887958390965\n",
      "142\n",
      "-11.378862400395093\n",
      "142\n",
      "-13.349185748405176\n",
      "142\n",
      "-14.759733284168647\n",
      "Episode 30\tAverage Score: 9.50\n",
      "142\n",
      "-13.576445959505406\n",
      "142\n",
      "-13.23771968963959\n",
      "142\n",
      "-13.039309500594385\n",
      "142\n",
      "-12.502060693313581\n",
      "142\n",
      "-14.650952640216644\n",
      "142\n",
      "-7.249565944081297\n",
      "142\n",
      "-11.827400971144613\n",
      "142\n",
      "-17.29474581175853\n",
      "142\n",
      "-13.601351516428991\n",
      "142\n",
      "-9.319171689541434\n",
      "Episode 40\tAverage Score: 9.38\n",
      "142\n",
      "-15.25033190907098\n",
      "142\n",
      "-11.982457377020722\n",
      "142\n",
      "-9.413331131738431\n",
      "142\n",
      "-14.417510288198287\n",
      "142\n",
      "-14.52364263799052\n",
      "142\n",
      "-8.969681404032485\n",
      "142\n",
      "-11.7159075579373\n",
      "142\n",
      "-15.520258061046405\n",
      "142\n",
      "-11.855924704537784\n",
      "142\n",
      "-11.954480634933532\n",
      "Episode 50\tAverage Score: 9.36\n",
      "142\n",
      "-11.246477108738844\n",
      "142\n",
      "-14.359365853858264\n",
      "142\n",
      "-15.135027668232368\n",
      "142\n",
      "-14.235714460479812\n",
      "142\n",
      "-12.451073792456071\n",
      "142\n",
      "-15.73974221034129\n",
      "142\n",
      "-13.472688061208999\n",
      "142\n",
      "-13.447614639967753\n",
      "142\n",
      "-12.889119346642843\n",
      "142\n",
      "-14.2624074788246\n",
      "Episode 60\tAverage Score: 9.33\n",
      "142\n",
      "-10.003942071551364\n",
      "142\n",
      "-11.758180686670443\n",
      "142\n",
      "-14.145493264775183\n",
      "142\n",
      "-10.732591098122384\n",
      "142\n",
      "-14.879868493716856\n",
      "142\n",
      "-13.982502633647897\n",
      "142\n",
      "-14.824322713131352\n",
      "142\n",
      "-12.028831937500627\n",
      "142\n",
      "-12.24756428726235\n",
      "142\n",
      "-11.948785264595529\n",
      "Episode 70\tAverage Score: 9.34\n",
      "142\n",
      "-13.978503108969221\n",
      "142\n",
      "-10.293085782438851\n",
      "142\n",
      "-12.969880019164975\n",
      "142\n",
      "-14.060533521490953\n",
      "142\n",
      "-11.328192293657679\n",
      "142\n",
      "-12.9073881666786\n",
      "142\n",
      "-12.474029785292657\n",
      "142\n",
      "-11.381095987116538\n",
      "142\n",
      "-12.136588176092033\n",
      "142\n",
      "-11.899129750717975\n",
      "Episode 80\tAverage Score: 9.35\n",
      "142\n",
      "-11.001037169416321\n",
      "142\n",
      "-11.536715266116914\n",
      "142\n",
      "-14.289652313735274\n",
      "142\n",
      "-12.75623676172458\n",
      "142\n",
      "-13.383768223698638\n",
      "142\n",
      "-9.728953731869183\n",
      "142\n",
      "-13.93659148287827\n",
      "142\n",
      "-14.468900502635961\n",
      "142\n",
      "-10.480709933804405\n",
      "142\n",
      "-11.678691993360928\n",
      "Episode 90\tAverage Score: 9.34\n",
      "142\n",
      "-12.84366619724321\n",
      "142\n",
      "-12.434698414746379\n",
      "142\n",
      "-14.128843353096158\n",
      "142\n",
      "-12.295754831709417\n",
      "142\n",
      "-12.560965151065176\n",
      "142\n",
      "-12.464849751056903\n",
      "142\n",
      "-16.546199791777184\n",
      "142\n",
      "-9.614195535082843\n",
      "142\n",
      "-12.54266159852989\n",
      "142\n",
      "-13.706532800450304\n",
      "Episode 100\tAverage Score: 9.32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def cem(n_iterations=100, max_t=200, gamma=1, print_every=10, pop_size=50, elite_frac=0.3, sigma=0.5):\n",
    "    \"\"\"PyTorch implementation of the cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        n_iterations (int): maximum number of training iterations\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        pop_size (int): size of population at each iteration\n",
    "        elite_frac (float): percentage of top performers to use in update\n",
    "        sigma (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    n_elite=int(pop_size*elite_frac)\n",
    "    agent = Agent(env).to(device)\n",
    "\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "#     best_weight = sigma*np.random.randn(agent.get_weights_dim())\n",
    "    #print(\"rrrrrrrrr\",agent.get_weights_dim())\n",
    "    \n",
    "    \n",
    "    #MCMC variables\n",
    "    pos_w = np.ones((n_iterations, agent.get_weights_dim()))\n",
    "    pos_tau = np.ones((n_iterations, 1))\n",
    "\n",
    "    rmse_train = np.zeros(n_iterations)\n",
    "    rmse_test = np.zeros(n_iterations)\n",
    "\n",
    "\n",
    "\n",
    "    w = sigma*np.random.randn(agent.get_weights_dim())\n",
    "    w_proposal = np.random.randn(agent.get_weights_dim())\n",
    "\n",
    "\n",
    "    step_w = 0.02  # defines how much variation you need in changes to w\n",
    "    step_eta = 0.01\n",
    "\n",
    "\n",
    "    #now declare the nn and do aforward pass\n",
    "    weights_pop = [w + (sigma*np.random.randn(agent.get_weights_dim())) for i in range(n_iterations)]\n",
    "    loss= np.array([agent.evaluate(weights, gamma, max_t) for weights in weights_pop])\n",
    "#     print(\"before normalization = \",rewards)\n",
    "#     print(\"rewards \",normalize(rewards[:,np.newaxis], axis=0).ravel())\n",
    "    \n",
    "    tau_pro = np.var(loss)\n",
    "    if tau_pro==0:\n",
    "        eta=0.000001\n",
    "    else:\n",
    "        eta = np.log(np.var(loss))\n",
    "    \n",
    "    \n",
    "#     print(\"tau is\",tau_pro)\n",
    "#     print(\"eta is\",eta)\n",
    "    sigma_squared = 25\n",
    "    nu_1 = 0\n",
    "    nu_2 = 0\n",
    "    prior_likelihoo = prior_likelihood(sigma_squared, nu_1, nu_2, w, tau_pro,agent.h_size,agent.s_size)  # takes care of the gradients\n",
    "#     print(\"prior likelihood=\",prior_likelihoo)\n",
    "    [likelihood, rmsetrain] = likelihood_func( sigma,gamma,max_t,w, tau_pro,n_iterations,loss)\n",
    "\n",
    "#     print(\"likelihood=\",likelihood)\n",
    "    naccept = 0\n",
    "    \n",
    "\n",
    "    for i_iteration in range(1, n_iterations+1):\n",
    "#         print(\"iter= \",i_iteration)\n",
    "        #weights_pop = [best_weight + (sigma*np.random.randn(agent.get_weights_dim())) for i in range(pop_size)]\n",
    "        #print(\"this is the real one time evalute output:\",agent.evaluate(best_weight, gamma, max_t))\n",
    "        #print(\"reward is\",rewards)\n",
    "        \n",
    "        \n",
    "        \n",
    "        w_proposal = w + np.random.normal(0, step_w,agent.get_weights_dim())\n",
    "        eta_pro = eta + np.random.normal(0, step_eta, 1)\n",
    "        tau_pro = math.exp(eta_pro)\n",
    "        \n",
    "        loss = (agent.evaluate(w_proposal, gamma, max_t))\n",
    "#         print(reward/i_iteration)\n",
    "#         norm_rew_pro = reward/200.0\n",
    "#         print(\"reward=\",reward)\n",
    "        [likelihood_proposal, rmsetrain] = likelihood_func(sigma,gamma,max_t,w_proposal, tau_pro,n_iterations,loss)\n",
    "      \n",
    "      #  print(\"likelihood proposal=\",likelihood_proposal)\n",
    "        prior_prop = prior_likelihood(sigma_squared, nu_1, nu_2, w_proposal, tau_pro,agent.h_size,agent.s_size)\n",
    "        diff_likelihood = likelihood_proposal - likelihood/n_iterations\n",
    "        \n",
    "        diff_priorliklihood = prior_prop - prior_likelihoo\n",
    "#         print(prior_prop,prior_likelihoo)\n",
    "        print(diff_likelihood+diff_priorliklihood)\n",
    "        \n",
    "#         print(\"sum= \",diff_likelihood)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mh_prob = min(0, (diff_likelihood+diff_priorliklihood))\n",
    "#         print(\"metropolis hastings probablitiy = \",mh_prob)\n",
    "        \n",
    "        u = random.uniform(0, 1)\n",
    "        if u < mh_prob:\n",
    "            # Update position\n",
    "            #print(i, ' is the accepted sample')\n",
    "            naccept += 1\n",
    "            \n",
    "            w = w_proposal\n",
    "            eta = eta_pro\n",
    "                    \n",
    "            # print i, 'rejected and retained'\n",
    "\n",
    "        rewards = agent.evaluate_return(w, gamma=1.0)\n",
    "        scores_deque.append(rewards)\n",
    "        scores.append(rewards)       \n",
    "        if i_iteration % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "\n",
    "        if np.mean(scores_deque)>=90.0:\n",
    "            print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "        \n",
    "#         elite_idxs = rewards.argsort()[-n_elite:]\n",
    "#         elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "#         best_weight = np.array(elite_weights).mean(axis=0)\n",
    "\n",
    "#         reward = agent.evaluate(best_weight, gamma=1.0)\n",
    "#         scores_deque.append(reward)\n",
    "#         scores.append(reward)\n",
    "        \n",
    "#         torch.save(agent.state_dict(), 'checkpoint.pth')\n",
    "        \n",
    "#         if i_iteration % print_every == 0:\n",
    "#             print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "\n",
    "#         if np.mean(scores_deque)>=90.0:\n",
    "#             print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-100, np.mean(scores_deque)))\n",
    "#             break\n",
    "#     return scores\n",
    "\n",
    "scores = cem()\n",
    "\n",
    "# # plot the scores\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEGCAYAAACNaZVuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAcEUlEQVR4nO3deZhU9Z3v8fcXmh2kaWlbZLExoggaFtuFMTqOiBqN6ExUXK4SY4YbzY0xztwJRm+MjjOPyU3c7sSJuCSYBXfF6OMGkrgjjaIgiOzK3iCLsjbwvX/Ur6FsqquLpk8V1O/zep566pzfOVXne+rAp07/6izm7oiISFxaFLoAERHJP4W/iEiEFP4iIhFS+IuIREjhLyISoZJCF5CLrl27emVlZaHLEBHZr0ydOnWVu5dnmrZfhH9lZSXV1dWFLkNEZL9iZosamqZuHxGRCCn8RUQipPAXEYmQwl9EJEIKfxGRCCn8RUQipPAXEYmQwj/42yc1fPb5xkKXISKSF/vFSV75MPKhd2nZwpj3n2cXuhQRkcRpzz/N9h26sY2IxCHq8J+78kte+mh5ocsQEcm7qLt9Tr/jbwAsvP2cAlciIpJfUe/5i4jESuEvIhIhhb+ISIQU/iIiEVL4i4hESOEvIhIhhb+ISIQU/iIiEUos/M3sSDOblvZYb2bXmVmZmb1iZnPCc5ekahARkcwSC393n+3uA919IHAssBF4GhgNTHT3PsDEMC4iInmUr26focA8d18EnAeMDe1jgfPzVIOIiAT5Cv+LgXFhuMLdl4Xh5UBFpheY2Sgzqzaz6pqamnzUKCISjcTD38xaA8OBx+tPc3cHMl5H2d3HuHuVu1eVl5cnXKWISFzysef/TeA9d18RxleYWTeA8LwyDzWIiEiafIT/Jezq8gF4FhgZhkcC4/NQg4iIpEk0/M2sAzAMeCqt+XZgmJnNAU4P4yIikkeJ3szF3TcAB9ZrW03q6B8RESkQneErIhIhhb+ISIQU/iIiEVL4i4hESOEvIhIhhb+ISIQU/iIiEVL4i4hESOEvIhIhhb+ISIQU/iIiEVL4i4hESOEvIhIhhb+ISIQU/iIiEVL4i4hESOEvIhIhhb+ISIQU/iIiEVL4i4hEKNHwN7NSM3vCzD42s1lmNsTMyszsFTObE567JFmDiIjsLuk9/7uBF929LzAAmAWMBia6ex9gYhgXEZE8Siz8zawzcArwIIC7b3X3tcB5wNgw21jg/KRqEBGRzJLc8+8N1AC/M7P3zewBM+sAVLj7sjDPcqAi04vNbJSZVZtZdU1NTYJliojEJ8nwLwEGA//t7oOADdTr4nF3BzzTi919jLtXuXtVeXl5gmWKiMQnyfBfDCx298lh/AlSXwYrzKwbQHhemWANIiKSQWLh7+7Lgc/M7MjQNBSYCTwLjAxtI4HxSdUgIiKZlST8/j8E/mRmrYH5wJWkvnAeM7OrgEXARQnXICIi9SQa/u4+DajKMGlokssVEZHsdIaviEiEFP4iIhFS+Nczef7qQpcgIpI4hX89I8a8w2efbyx0GSIiiVL4Z7Bh67ZClyAikiiFv4hIhBT+IiIRUviLiERI4S8iEiGFv4hIhBT+GRhW6BJERBKl8M+gdvuOQpcgIpIohX8G9702v9AliIgkSuGfwfpNtYUuQUQkUQp/YPuOjHeSFBEpWgp/4C8fLC10CSIieaXwB7Zs217oEkRE8krhn4E6gUSk2Cn8RUQipPDPQKd4iUixU/iLiESoJMk3N7OFwBfAdmCbu1eZWRnwKFAJLAQucvc1SdYhIiJflY89/39w94HuXhXGRwMT3b0PMDGMF5Su5SMisSlEt895wNgwPBY4P58Lf+q9xTwxdfFX2lzH94hIZBLt9iF11OTLZubAfe4+Bqhw92Vh+nKgItMLzWwUMAqgV69ezVbQ9Y99AMAFx/ZotvcUEdnfJB3+33D3JWZ2EPCKmX2cPtHdPXwx7CZ8UYwBqKqqSnTXXN0+IhKbRLt93H1JeF4JPA0cD6wws24A4XllkjU0hTqBRKTYJRb+ZtbBzDrVDQNnADOAZ4GRYbaRwPikahARkcyS7PapAJ42s7rl/NndXzSzKcBjZnYVsAi4KMEamkSdQCJS7BILf3efDwzI0L4aGJrUckVEpHE6w1dEJEIKfxGRCEUb/g+9saDQJYiIFEy04X/rczMLXYKISMFEG/4iIjFT+MNux3bqJC8RKXYKf1Dai0h0FP4Z6CQvESl2OYe/mX3DzK4Mw+Vm1ju5svJMaS8ikckp/M3sZuAnwA2hqRXwx6SKEhGRZOW65/+PwHBgA4C7LwU6JVVU3qnPX0Qik2v4b3V3J8RkuEpn0dJ3gYgUu1zD/zEzuw8oNbN/BiYA9ydXVp6pz19EIpPTVT3d/VdmNgxYDxwJ/MzdX0m0sgLSd4GIFLtGw9/MWgIT3P0fgKIN/HTq9hGRYtdot4+7bwd2mFnnPNSzT9Cev4gUu1xv5vIlMN3MXiEc8QPg7tcmUpWIiCQq1/B/KjxERKQI5PqD71gzaw0cEZpmu3ttcmXll7p5RCQ2OYW/mZ0KjAUWksrKnmY20t1fS660/NEPvCISm1y7fX4NnOHuswHM7AhgHHBsYy8MRwtVA0vc/VvhmkCPAAcCU4HL3X1rU4pPir4MRKTY5XqSV6u64Adw909IXd8nFz8CZqWN/wK4090PB9YAV+X4PolRt4+IxCbX8K82swfM7NTwuJ/U3nxWZtYDOAd4IIwbcBrwRJhlLHD+npedLH0ZiEixy7Xb52rgB0DdoZ2vA/fm8Lq7gH9j10XgDgTWuvu2ML4Y6J5jDYlRN4+IxCbX8C8B7nb3O2BnP36bbC8ws28BK919avjBeI+Y2ShgFECvXr329OUiIpJFrt0+E4F2aePtSF3cLZuTgOFmtpDUD7ynAXeTujhc3ZdOD2BJphe7+xh3r3L3qvLy8hzLzO7T1RsztqubR0Rik2v4t3X3L+tGwnD7bC9w9xvcvYe7VwIXA6+6+2XAJOCCMNtIYPweV91Ep/zfSflalIjIPi3X8N9gZoPrRsysCtjUxGX+BLjezOaS+g3gwSa+j4iINFGuff7XAY+b2dIw3g0YketC3P2vwF/D8Hzg+NxLFBGR5pZ1z9/MjjOzg919CtAXeBSoBV4EFuShvoLQ0T8iUuwa6/a5D6g7+3YI8FPgN6ROzhqTYF0iIpKgxrp9Wrr752F4BDDG3Z8EnjSzacmWlj+pc8/SxgtUh4hIvjS2598y7bDMocCradNy/b1gn5e6N72ISDwaC/BxwN/MbBWpo3teBzCzw4F1CdcmIiIJyRr+7v4fZjaR1NE9L/uuXeQWwA+TLi5f6nf7iIgUu0a7btz9nQxtnyRTTmGs/nJLoUsQEcmrXE/yKmobtmxrfCYRkSISTfhn+1G3dod+8BWRuEQT/tlsrxf++ioQkWKn8BcRiZDCn927hHTsj4gUO4W/iEiEogn/bCfx1p+mPn8RKXbRhH829c/xWreptjCFiIjkicI/gw8+W1voEkREEqXwJ3uXkIhIMVL4i4hEKJrw1869iMgu0YR/NvpiEJHYKPxRn7+IxCex8Deztmb2rpl9YGYfmdktob23mU02s7lm9qiZtU6qhqbq2rFNoUsQEUlUknv+W4DT3H0AMBA4y8xOBH4B3Onuh5O6EfxVCdbQJL3K2hW6BBGRRCUW/p7yZRhtFR4OnAY8EdrHAucnVUOuHF3VU0Tikmifv5m1NLNpwErgFWAesNbd6+6eshjo3sBrR5lZtZlV19TU7HUt2a7nv9vlHZT+IlLkEg1/d9/u7gOBHsDxQN89eO0Yd69y96ry8vLEasy47LwuTUQk//JytI+7rwUmAUOAUjOru3dwD2BJPmrIpv5fBdn+ShARKQZJHu1TbmalYbgdMAyYRepL4IIw20hgfFI1iIhIZiWNz9Jk3YCxZtaS1JfMY+7+nJnNBB4xs9uA94EHE6whJ9rPF5HYJBb+7v4hMChD+3xS/f95lS3g9YOviMRGZ/iy+/X8RUSKncKfTHfy0q6/iBQ3hT8ZTvJS9otIkVP4oz5/EYlPNOGf9QbujYyLiBSbaMJ/T+gkLxEpdgp/1M0jIvFR+IuIREjhLyISIYU/UP8nXnUDiUixiyb8s524pZO8RCQ20YR/NjrOX0Rio/BHt3EUkfgo/DPQcf4iUuyiCf93F3xe6BJERPYZ0YT/xFkrc55X+/0iUuyiCf9sduvlUfqLSJFT+AM7FPYiEhmFfwb6LhCRYqfwz2Dj1m2FLkFEJFGJhb+Z9TSzSWY208w+MrMfhfYyM3vFzOaE5y5J1ZAu2+Gb9Y/zX7F+S9LliIgUVJJ7/tuAf3H3fsCJwA/MrB8wGpjo7n2AiWFcRETyKLHwd/dl7v5eGP4CmAV0B84DxobZxgLnJ1VDztTJLyKRyUufv5lVAoOAyUCFuy8Lk5YDFfmoQUREdkk8/M2sI/AkcJ27r0+f5qmO+Iz73WY2ysyqzay6pqYm0Rq14y8isUk0/M2sFang/5O7PxWaV5hZtzC9G5Dx1Ft3H+PuVe5eVV5evte1ZAt4XctHRGKT5NE+BjwIzHL3O9ImPQuMDMMjgfFJ1ZBO+S4isktJgu99EnA5MN3MpoW2nwK3A4+Z2VXAIuCiBGsQEZEMEgt/d38DsAYmD01quU2hyzuISGx0hi/6wVdE4qPwFxGJUDThr5uyi4jsEk34Z6NDPUUkNgp/EZEIRRP+2Xbutd8vIrGJJvyzUvqLSGSiCX/lu4jILtGEfzY6EkhEYqPwB9qWtCx0CSIieaXwB1q0aOgqFCIixUnhj674KSLxUfijPn8RiU804Z91717ZLyKRiSb8lfAiIrtEFP4iIlJH4S8iEqFowv+jpesbnKYOIRGJTTTh/+HidQ1O0yWdRSQ20YR/Nop+EYlNYjdw3xe8NW8Vl94/meMqu+zxaytHP09p+1Z0bFPChOv/nrsnzuGKIYdyYIc2HHHTCxxfWcaoUw7j4+XrOarbAUyavZJ+3Trz+7cWcOGxPVm3qZbS9q3oXtqOrdt30KltCf/rz++zcev2ncs4+IC2/HhYHx5+exElLYxzBxzCbc/PAuDfz+vP5UMqmfTxSq78/RQAfnPpYM4+5mDunjiHeyfNY+v2HTvf6zt/V8kpR3Tl4bcXUXlgB24+tx9H3/wStTucT277JivWb+bHj07jtL4HsX7zNu6ZOIfTj6rg+39/GF/vUcqNT0/HgV5l7dm2fQfjP1jK8AGH0PfgA5i5bB1/+WAZn36+keEDDuGeSwbxwOvzOfygjjw+dTHTF69j/eZajq8s4+DObVm+bjMvz1yx22d6cp+u7HDnzbmr6dyuFes21dKv2wE8f+03uPW5mbw0YzlL121m6k2nM3vFF0yYuZKH3lwAwC3D+9PnoI4c3Lktl94/meXrN9O5XSsG9CzloyXrqN2+g4evOoH/+YdqWrVsweI1mzi6+wHcMvxojj20Cyu/2MyDry9g8ZpNrN20lTfnruba0w7nnlfn7vys35iziu/+fgr3XjaY7z1cvbPubp3bMuqUw7jlLzMB+OFph/P/Xp0LwD8N6s7Nw/sz4JaXAbj3ssHc+PR01mysTU0f3J2n3lsCkNq+5x/N3RPm0PfgTtz0zIyvbMPpPz+DsW8tZMxr8/nOSb25ftgRrP5yC8feNgGAP151AgtXb+CmZ2ZwZv8K7ru8CoBx737KDU9N3/k+Z/av4Lsn9eaFGctZtm4THy1dz6hTDqO0fWtueno61w7tw3GVZbQwY/hv3uD0oyq4c8RAOrZJxUHVbRNY9eUW7hoxkE8/38hDby5gbVgfgEM6t2Xpus20btmCQ0rbcmDHNlxz6tcYelQFy9dt5ndvLeD8gd258enpvPfpWgB+deEA5tV8yf8+48ivnFE/7t1P6d21A49O+YwvNm+jc7tWtGnVgp+f259Fqzcw7M7X6FXWnm8P7sGT7y3m0883AnBM984c3b0zrVsaZsZb81bxyYovAehZ1o7PPt+0cxlf79H5K3/5jzrlMJas2USLFkbbkhY8PnUxd40YyIJVGyjv1IYjD+7E98ZW065VSwYfWsoRFZ24a8Ichhx2IG/PX83bN5zGXz5YytK1m+nUtmTnvwWA7qXtOKZ7Z6oXrWHT1m10atuKi4/vyV0T5gBwfGUZc1Z+wZqNtZR1aM3nG7ZyQu8yOrVtxQ1n9+UXL3xM+9YtmbJwDW1KWvD8tSfTrnUyl5+xpLo8zOwh4FvASnc/OrSVAY8ClcBC4CJ3X9PYe1VVVXl1dXVjs+2mcvTzOc133sBDGD9taYPTLz6uJ49M+YwTepdxUVVP/uXxD/a4lqZYePs5u63DlBtP57j/mNDoa1+87mTOuut1AGbeeiajHp7KG3NXZZz3VxcO4F/3YJ0+/vez6Pt/Xsx5/sZMven0nQEHcMoR5bz2SU3GebuXtmPJ2k0ZpzVk4e3n8M8PV/NKhi+k9Hly/fdS3xEVHXcGT2MuP/FQ/vDOoozT6gIuvabvjZ3ChFkrM86/8PZzgNz/nWdz9alf4ydn9WXj1m30+9lLTXqPhbefw2UPvMObc1c3OM/T1/wdg3rt2hlrqPZfXziAW5+bybpNtRmnx+InZ/Xl6lO/1uTXm9lUd6/KNC3Jbp/fA2fVaxsNTHT3PsDEMF5wjX3/bapN7a1v2baDLdt2ZJ85YU35st62w9mape7tO/ZsnWq3N+9nYPbVaytt3ba9gTlp8uff3DWnq/v3kYts22FzhvfZtiM/nZJ1dbWwvbvOVe227PXmujYObNqa++darPb0/+aeSGzPH8DMKoHn0vb8ZwOnuvsyM+sG/NXdj2zsfZLe8xcR2VfV/YXXFIXa88+kwt2XheHlQEVDM5rZKDOrNrPqmprMXQB7Y+SQQ5v9PUVEmtv2hP76K9jRPp76k6PBtXL3Me5e5e5V5eXlzb78n53bv9nfU0Skue1IqHcm3+G/InT3EJ4z/5IlIiIALFq9MZH3zXf4PwuMDMMjgfF5Xj4At57Xn5a6gYuI7AdWrt+cyPsmFv5mNg54GzjSzBab2VXA7cAwM5sDnB7G82pgz1KuGFKZ78WKiDTJIaXtEnnfxE7ycvdLGpg0NKll5qJdK92vV0T2H0n1UhT15R3uvWzwbm13XTxw5/CvLxzwlWmXntCLYf0qaJ/QGXV7qnfXDoUuYadr9uJEk/1RaftWGdtP7tM1z5Uk6+pTv8agXqWJL6d1SQsG9OjMIZ3b7my74NgeiS+3JI/du2f2b/DgxZ2+PXjP17lHl2T2/BM9zr+5NPU4fxGRmO1Lx/mLiMg+QOEvIhIhhb+ISIQU/iIiEVL4i4hESOEvIhIhhb+ISIQU/iIiEdovTvIysxog873vGtcVyHz/wuKldY6D1rn47e36HuruGa+Jv1+E/94ws+qGznArVlrnOGidi1+S66tuHxGRCCn8RUQiFEP4jyl0AQWgdY6D1rn4Jba+Rd/nLyIiu4thz19EROpR+IuIRKiow9/MzjKz2WY218xGF7qepjKznmY2ycxmmtlHZvaj0F5mZq+Y2Zzw3CW0m5ndE9b7QzMbnPZeI8P8c8xsZKHWKVdm1tLM3jez58J4bzObHNbtUTNrHdrbhPG5YXpl2nvcENpnm9mZhVmT3JhZqZk9YWYfm9ksMxtS7NvZzH4c/l3PMLNxZta22LazmT1kZivNbEZaW7NtVzM71symh9fcY2aN38LM3YvyAbQE5gGHAa2BD4B+ha6rievSDRgchjsBnwD9gF8Co0P7aOAXYfhs4AXAgBOByaG9DJgfnruE4S6FXr9G1v164M/Ac2H8MeDiMPxb4OowfA3w2zB8MfBoGO4Xtn0boHf4N9Gy0OuVZX3HAt8Lw62B0mLezkB3YAHQLm37fqfYtjNwCjAYmJHW1mzbFXg3zGvhtd9stKZCfygJfthDgJfSxm8Abih0Xc20buOBYcBsoFto6wbMDsP3AZekzT87TL8EuC+t/Svz7WsPoAcwETgNeC78w14FlNTfxsBLwJAwXBLms/rbPX2+fe0BdA5BaPXai3Y7h/D/LARaSdjOZxbjdgYq64V/s2zXMO3jtPavzNfQo5i7fer+UdVZHNr2a+HP3EHAZKDC3ZeFScuBujtIN7Tu+9tnchfwb8COMH4gsNbdt4Xx9Pp3rluYvi7Mvz+tc2+gBvhd6Op6wMw6UMTb2d2XAL8CPgWWkdpuUynu7VynubZr9zBcvz2rYg7/omNmHYEngevcfX36NE995RfNcbtm9i1gpbtPLXQteVRCqmvgv919ELCBVHfATkW4nbsA55H64jsE6ACcVdCiCqAQ27WYw38J0DNtvEdo2y+ZWStSwf8nd38qNK8ws25hejdgZWhvaN33p8/kJGC4mS0EHiHV9XM3UGpmJWGe9Pp3rluY3hlYzf61zouBxe4+OYw/QerLoJi38+nAAnevcfda4ClS276Yt3Od5tquS8Jw/fasijn8pwB9wlEDrUn9OPRsgWtqkvDL/YPALHe/I23Ss0DdL/4jSf0WUNd+RThq4ERgXfjz8iXgDDPrEva4zght+xx3v8Hde7h7Jalt96q7XwZMAi4Is9Vf57rP4oIwv4f2i8NRIr2BPqR+HNvnuPty4DMzOzI0DQVmUsTbmVR3z4lm1j78O69b56LdzmmaZbuGaevN7MTwGV6R9l4NK/SPIAn/wHI2qSNj5gE3FrqevViPb5D6k/BDYFp4nE2qr3MiMAeYAJSF+Q34TVjv6UBV2nt9F5gbHlcWet1yXP9T2XW0z2Gk/lPPBR4H2oT2tmF8bph+WNrrbwyfxWxyOAqiwOs6EKgO2/oZUkd1FPV2Bm4BPgZmAH8gdcROUW1nYByp3zRqSf2Fd1VzblegKnx+84D/ot5BA5keuryDiEiEirnbR0REGqDwFxGJkMJfRCRCCn8RkQgp/EVEIqTwl6JmZtvNbFraI+vVXc3s+2Z2RTMsd6GZdW3C6840s1vCFR9f2Ns6RBpS0vgsIvu1Te4+MNeZ3f23SRaTg5NJneB0MvBGgWuRIqY9f4lS2DP/ZbgG+rtmdnho/7mZ/WsYvtZS91D40MweCW1lZvZMaHvHzL4e2g80s5ctdV36B0idqFO3rP8RljHNzO4zs5YZ6hlhZtOAa0ld0O5+4Eoz2y/PSpd9n8Jfil27et0+I9KmrXP3Y0idEXlXhteOBga5+9eB74e2W4D3Q9tPgYdD+83AG+7eH3ga6AVgZkcBI4CTwl8g24HL6i/I3R8ldbXWGaGm6WHZw/dm5UUaom4fKXbZun3GpT3fmWH6h8CfzOwZUpdagNSlNr4N4O6vhj3+A0jdrOOfQvvzZrYmzD8UOBaYEm6u1I5dF/Cq7whSN+gA6ODuX+SwfiJNovCXmHkDw3XOIRXq5wI3mtkxTViGAWPd/YasM5lVA12BEjObCXQL3UA/dPfXm7BckazU7SMxG5H2/Hb6BDNrAfR090nAT0hdOrgj8Dqh28bMTgVWeereCq8Bl4b2b5K6IBukLtx1gZkdFKaVmdmh9Qtx9yrgeVLXtv8lqQsRDlTwS1K05y/Frl3Yg67zorvXHe7Zxcw+BLaQuvVdupbAH82sM6m993vcfa2Z/Rx4KLxuI7suyXsLMM7MPgLeInWpYtx9ppndBLwcvlBqgR8AizLUOpjUD77XAHdkmC7SbHRVT4lSuElMlbuvKnQtIoWgbh8RkQhpz19EJELa8xcRiZDCX0QkQgp/EZEIKfxFRCKk8BcRidD/B1xoqtn2AZGMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "state = env.reset()\n",
    "while True:\n",
    "    state = torch.from_numpy(state).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        action = agent(state)\n",
    "        \n",
    "    env.render()\n",
    "    action = (round(((max(action))).item()))\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
